{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User interface to implement 2 deep learning models by using the webcam \n",
    "---\n",
    "This code implements the 2 deep learning models that are both trained to recognize human emotion by analyzing an image of the human face with the difference that one has been trained with Transfer Learning and the other is trained from scratch\n",
    "\n",
    "---\n",
    "Class: Machine Learning\n",
    "\n",
    "By: Sebastian Russo, Guillermo Trigo, Javier Peres\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import tensorflow as tf           #Open source library for AI, ML and DL\n",
    "#from tensorflow import image      #For image processing and manipulation\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "import numpy as np                #For numpy arrays and for scientific computing in Python\n",
    "import cv2                        #OpenCV to perform operations on images and videos with the webcam \n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters and global variables\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from configuration.config_invoke import json_data\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Constants and general variables\n",
    "#Fixed images measurements\n",
    "img_height = json_data[\"images\"][\"height\"]\n",
    "img_width = json_data[\"images\"][\"width\"]\n",
    "\n",
    "#7 classes for seven emotions and of course 2 arrays with the labels and actual emojis\n",
    "class_labels  = json_data[\"classes\"][\"labels\"]\n",
    "class_label_emojis = json_data[\"classes\"][\"emojis\"]\n",
    "\n",
    "directory_name = json_data[\"dl_model\"][\"directory\"]\n",
    "no_transfer_learning_name = json_data[\"dl_model\"][\"no_transfer_learning\"][\"name\"]\n",
    "transfer_learning_name = json_data[\"dl_model\"][\"transfer_learning\"][\"name\"]\n",
    "\n",
    "#Load the pre-trained models into the environment\n",
    "model_no_transfer_learning = tf.keras.models.load_model(\n",
    "    f'{directory_name}/{no_transfer_learning_name}') \n",
    "model_transfer_learning = tf.keras.models.load_model(\n",
    "    f'{directory_name}/{transfer_learning_name}')   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to preprocess images and make predictions\n",
    "---\n",
    "Must be the exact same preprocessing as the one used during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprocess a single image\n",
    "def pre_process_single_image(img, target_size):\n",
    "    img=cv2.resize(img, target_size)  #Resize image to target size\n",
    "    \n",
    "    #img_array=image.img_to_array(img) #Convert the image to an array format\n",
    "    img_array = img_to_array(img)\n",
    "    \n",
    "    #Preprocess array with denseNet preprocessing function\n",
    "    img_array= preprocess_input(img_array)     \n",
    "    img_array = img_array * (1. / 255) #Rescale pixels to range [0,1]\n",
    "    img_array = np.expand_dims(img_array, axis=0)   #Expand dimensions to add batch\n",
    "    \n",
    "    return img_array #Return the preprocessed image array\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "#Function to predict using the selected model\n",
    "def predict_emotion(model, preprocessed_img):\n",
    "    prediction = model.predict(preprocessed_img)        #Generate prediction with selected model\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]  #Find index of most probable class \n",
    "    predicted_label = class_labels[predicted_class]     #Retrieve label corresponding to the predicted class index\n",
    "    \n",
    "    return predicted_label                              #Return the predicted label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize CV2 for video capture\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Correct way to define codec\n",
    "four_cc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "# Create the video writer object\n",
    "out = cv2.VideoWriter('captured_faces_video.avi', four_cc, 20.0, (640, 480))\n",
    "\n",
    "#Load the pre-trained face detector from OpenCV\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute window and use selected model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt user to choose a model\n",
    "print(\"Choose a model:\")\n",
    "print(\"1. Model without Transfer Learning\")\n",
    "print(\"2. Model with Transfer Learning\")\n",
    "choice = input(\"Enter your choice (1 or 2): \")\n",
    "\n",
    "if choice == '1':\n",
    "    selected_model = model_no_transfer_learning\n",
    "    print(\"Using Model without Transfer Learning\")\n",
    "elif choice == '2':\n",
    "    selected_model = model_transfer_learning\n",
    "    print(\"Using Model with Transfer Learning\")\n",
    "else:\n",
    "    print(\"Invalid choice. Using Model without Transfer Learning by default.\")\n",
    "    selected_model = model_no_transfer_learning\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    #Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Loop through detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        #Extract the region of interest (ROI)\n",
    "        roi_color = frame[y:y + h, x:x + w]\n",
    "\n",
    "        #Preprocess the face image\n",
    "        preprocessed_img = pre_process_single_image(roi_color, target_size=(img_height, img_width))\n",
    "\n",
    "        #Make a prediction using the selected model\n",
    "        predicted_label = predict_emotion(selected_model, preprocessed_img)\n",
    "\n",
    "        #Display the label and bounding box on the frame\n",
    "        label = f'{predicted_label}'\n",
    "        cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    #Write the frame with predictions to the video file\n",
    "    out.write(frame)\n",
    "\n",
    "    #Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    #Check for key press\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('c'):\n",
    "        #Save the image with the face box\n",
    "        cv2.imwrite('captured_face_image_with_label.jpg', frame)\n",
    "        print(\n",
    "            \"Image captured and saved as 'captured_face_image_with_label.jpg'\")\n",
    "\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "#Release video capture and writer objects\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
